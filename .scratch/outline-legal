My outline goes like this:

* MT began shortly after computers were invented
* Weaver proposed using code-breaking techniques
* IBM-Georgetown - rule-based (=dictionary + grammar): RBMT dominates
* IBM CANDIDE (/Canadian Hansard) - based on methods proposed by
Weaver: p(f|e)p(e) (probability of French being a translation of
English; probability of English in context)
** "Chinese menu" - the symbol that appears in Chinese wherever 'soup'
appears in English probably means 'soup'.
* PBMT - beyond 1:1 word alignments

* Purpose: assimilation vs. dissemination (gist vs. publication: free
services vs. (e.g.) the systems used by the EU)

* Problem: sparse data! -> "The only thing better than data is more data"
** Web crawling (Common Crawl, etc.)
** Crowd sourcing (Google Translate community, Amazon Mechanical Turk, etc.)

*Problem: blind alignment - documents are not just translated, they
are localised
** English becomes Spanish ('for information in English' is not
typically relevant to a Spanish-speaking audience)
** Dublin becomes Vienna ('contact our local offices in Dublin')
** Million becomes billion (short vs. long scale)
** East becomes West (Germany is east of England/America, but west of
Russia, etc.)
** "Currency conversion" (American documents will quote prices in
dollars; German, euros)
** Unit conversions (America uses pounds, Europe, grammes)
(I think two examples are enough, but I collect these :)

*Problem: blind trust
** "Translate Server Error"
** Fluency over fidelity: SMT is often deceptively fluent; users
assume more fluent == more accurate.
* Subproblem: discontiguous alignments (actually, a subproblem of data
sparseness)
** German split verbs: one or both parts can be dropped due to
language model penalties, changing the meaning
** Slavic verbs: negative verb phrases can become positive

*MT in the wild
** Most non-institutional use of MT is indirect: e-mail, search,
Facebook comments. With e-mail and comments in particular, MT tends to
perform poorly, as most corpora are written in formal language.

> Of particular interest in my view is, what exactly happens to the text data
> that the user inputs into the translator? - I assume this gets pinged to
> some remote repository for processing (or are there models where the data
> never leaves the user's computer/device?),

In the main, free services are remotely accessed, but RBMT systems are
typically local, and there are "lite" versions of Google/Bing
Translate for mobile devices that offer a limited SMT system, if the
network is unavailable.

> and once in the repository the
> text is read or at least available for reading/mining - including secondary
> uses - by the MT-provider or other parties they may pass the data on to?
>
> Here the typical MT business model would be interesting to explore - what
> practical uses do MT providers put the data to? (In the original paper Pawel
> distinguished uses for refining the translator itself, from uses to learn
> things about the user of the service  - or third parties who feature in the
> text - for marketing purposes). Does this reflect the actual position in
> your view?
>

I think so. With multi-service providers (Google, Microsoft, Yandex,
etc.), MT becomes just another input. Indirect uses of MT, in
particular, such as when Google offers to translate search results or
email, contribute equally to their advertising model of you as
untranslated searches or emails. The use of their translation service
is logged as part of your activity, just as every other service, and
much of their research advances are due to their ability to use data
collected for one purpose for another. They use language data
collected for search to enhance their MT; they use data collected for
MT to enhance other NLP tasks, such as parsing (this, at least, I can
cite).

Google's terms of service (which is pretty typical), contains this:
"When you upload, submit, store, send or receive content to or through
our Services, you give Google (and those we work with) a worldwide
license to use, host, store, reproduce, modify, create derivative
works (such as those resulting from translations, adaptations or other
changes we make so that your content works better with our Services),
communicate, publish, publicly perform, publicly display and
distribute such content. The rights you grant in this license are for
the limited purpose of operating, promoting, and improving our
Services, and to develop new ones."

> A further issue is what role, if any, do crowd-sourcing approaches play in
> MT? So, is it the case that some MT providers make available text to be
> translated to the 'translator community' to elicit different solutions, and
> produce a translation based on a statistical evaluation of the responses? If
> so, it would be interesting to know if the MT providers take steps to
> 'scrub' the text first - i.e. by deleting real names, etc.

Yes: Google had a blog post about this two days ago
(http://googletranslate.blogspot.ie/2015/06/google-translate-keeps-getting-better.html).
This is limited to two 'gamified' tasks: adding translations of
phrases, and assessing MT output. The text here is scrubbed.

There is also a second service provided by Google, the Translator's
Toolkit, which provides a Translation Memory system for translators.
The translation is pre-filled with Google's MT output, and the user's
translation (according to the old terms of service) is used to refine
Google's MT output. This is not scrubbed -- the translation input is
entirely at the user's discretion.

Finally, there is Google's facility to 'suggest an improvement' when
using Google Translate. Again, this is not scrubbed, and the input is
at the user's discretion.
