Statistical Machine Translation
Philipp Koehn
Cambridge University Press 2010



Ch 1 Introduction
1.2 History of Machine Translation

p16
.. use *interlingua* to represent meaning independent of a specific language.

The notion of representing meaning in a formal way tied together several strands of research both from artificial intelligence and computational linguistics.

.17
The problem of representing meaning in a formal way is one of the grand 
challenges of artificial intelligence with interesting philosophical
implications

p.18
traditional machine translation companies, such as the market leader Systran, are integrating statistical methods into their systems.

p14
1.2.1
Efforts to build machine translation systems started almost as soon as
electronic computers came into existence. Computers were used in Britain to
crack the German Enigma code in World War II and decoding language codes
seemed an apt metaphor for machine translation. Warren Weaver (Russian quote)

p355
11.3 Decoding by Parsing

Figure 11.10 Earley parsing algorithm adapted to decoding with synchronous 
grammars, which allows efficient lookup of all applicable rules

Input: Foreign sentence f = f_1 ... f_1_f, with syntax tree
Output: English translation e
1  for i=0 .. length(f)-1 do // initial chart
2    store pointer to initial node in prefix tree in span [i,i]
3  end for
4 for l=1..l_f do //build chart from the bottom up
5   for start=0 .. l_f - 1 do //beginning of span
6     end = start + 1
7     for midpoint=start .. end-1 do
8       for all dotted rules d in span [start,midpoint] do
9         for all distinct head node nonterminals or input words h covering
          span [midpoint+1,end] do
10          if extension d-> h exists in prefix tree then
11            d_{new} = d-> h
12            for all complete rules at d_{new} do
13              apply rules
14              store chart entries in span[start,end]
15            end for
16            if extension exist for d_{new} then
17              store d_{new} in span[start,end] // new dotted rule
18            end if
19          end if
20        end for
21      end for
22    end for
23  end for
24 end for
25 return English translation e from best chart entry in span [0, 1_f]

span[0,1] covers first word of input
we combine search points in the rule prefix tree with new spans
to separate the already processed span and the new span, we introduce a midpoint:
the processed span is [start,midpoint] and the new span is [midpoint+1,end]
loop through all possible midpoints starts in line 7
goal is to extend dotted rule in span[start, midpoint] with existing chart
entries in [midpoint+1, end]: continue search for rules in prefix tree given
head node labels or input words that exist in new span, for all dotted rules in
[start, midpoint]
three things may happen:
1. none of head node labels in span[midpoint+1,end] is in prefix tree => do
nothing
2a if match in prefix tree, we may reach complete rules. Apply rule, and add
new chart entries to the span[start, end]
2b. if find match in prefix tree, may reach active (dotted) rules. Store 
dotted rules (pointers into prefix tree) in the span [start, end]

The loops of spans and midpoints are cubic with respect to sentence length
Number of dotted rules is O(C^R) where C is no. of nonterminals and R the
rank of the grammar.
Large rank R is a problem.
Loop over nonterminals of the chart entries in a span is linear with total
no. of nonterminals and bounded by maximum stack size


11.3.8 Binarizing the grammar

(CFG -> CNF)
"Binarization of grammars is a well-known strategy in syntactic parsing."

"Not every synchronous grammar rule can be binarized. Take this example of a
4-ary rule:
  X -> A1 B2 C3 D4 | B2 D4 A1 C3

There is no way to split up the sequence of input nonterminals that allows a
corresponding split of the sequence of output nonterminals. While it is 
possible to binarize any monolingual grammar, this is not the case for
synchronous grammars. A 4-ary synchronous context grammar is more expressive
than a 3-ary grammar. Each higher rank after that enables new rules that
cannot be reduced to a lower-ranking grammar.



p.368

Syntactical tree-based models may be viewed as a probabilistic build-up to 
more complex machine translation models that use richly annotated syntactic or
dependency structures, or even some forms of interlingua. These systems are
usually built as a series of sequential steps [Zabkrtsky et al, 2008]. 
Conversely, starting from such complex models, probabilistic components may be
added. Using a traditional rule-based component for syntactic and semantic
analysis of the input and a generation component for the output allows the
training of translation models for mapping of f-structures [Riezler and
Maxwell, 2006]. Cowan et al [2006] propose a translation model that explicitly
models clause structure as aligned extended projections and that is trained
discriminatively.

p.368

Syntactical tree-based models may be viewed as a probabilistic build-up to 
more complex machine translation models that use richly annotated syntactic or
dependency structures, or even some forms of interlingua. These systems are
usually built as a series of sequential steps [Zabkrtsky et al, 2008]. 
Conversely, starting from such complex models, probabilistic components may be
added. Using a traditional rule-based component for syntactic and semantic
analysis of the input and a generation component for the output allows the
training of translation models for mapping of f-structures [Riezler and
Maxwell, 2006]. Cowan et al [2006] propose a translation model that explicitly
models clause structure as aligned extended projections and that is trained
discriminatively.

p.368

Syntactical tree-based models may be viewed as a probabilistic build-up to 
more complex machine translation models that use richly annotated syntactic or
dependency structures, or even some forms of interlingua. These systems are
usually built as a series of sequential steps [Zabkrtsky et al, 2008]. 
Conversely, starting from such complex models, probabilistic components may be
added. Using a traditional rule-based component for syntactic and semantic
analysis of the input and a generation component for the output allows the
training of translation models for mapping of f-structures [Riezler and
Maxwell, 2006]. Cowan et al [2006] propose a translation model that explicitly
models clause structure as aligned extended projections and that is trained
discriminatively.

...

Quirk and Corston-Oliver [2006] show that relatively small tree banks of only
25 sentences give decent performance, and while there are gains with larger
tree banks, they expect these gains to be diminishing with tree banks larger
than the ones currently available, i.e., over 40,000 sentences.

Zabokrtsky, Z., Ptacek, J., and Pajas, P. (2008) TectoMT: Highly modular MT
system with tectogrammatics used as transfer layer. In Proc. 3rd Workshop
on Statistical Machine Translation, pp. 167-170

Riezler, S., and Maxwell, J. T. (2006) Grammatical machine translation. In 
Proc. of the Human Language Technology Conference of the NAACL, Main 
Conference, pp 248-255.

Cowan, B., Kučerová, I., and Collins, M. (2006) A discriminative model for
tree-to-tree translation. In Proc. of the 2006 Conference on Empirical Methods
in Natural Language Processing, pp 232-241
